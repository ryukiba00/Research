{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"實作1_2020.08.14.ipynb","provenance":[],"authorship_tag":"ABX9TyMy2UB18uqMThXI2JwVckTd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xG-qozbQzErX","colab_type":"text"},"source":["## 參考"]},{"cell_type":"markdown","metadata":{"id":"_MMXMdfDy4Wo","colab_type":"text"},"source":["參考文章: Federated Learning: A Step by Step Implementation in Tensorflow  \n","參考網址: https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399"]},{"cell_type":"markdown","metadata":{"id":"HLKFO2BGzIFj","colab_type":"text"},"source":["## 實作"]},{"cell_type":"code","metadata":{"id":"OE8KzqTayn3K","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597561227971,"user_tz":-480,"elapsed":4147,"user":{"displayName":"王文友109","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdEALriVPsbWPOQKRGKGj2FgFjxLpKdAzbjSuv=s64","userId":"10727501453917481194"}}},"source":["# 匯入所有相關套件\n","import numpy as np\n","import random\n","import cv2\n","import os\n","from imutils import paths\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras import backend as K"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnzUhYZfzR2k","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597561260239,"user_tz":-480,"elapsed":1127,"user":{"displayName":"王文友109","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdEALriVPsbWPOQKRGKGj2FgFjxLpKdAzbjSuv=s64","userId":"10727501453917481194"}}},"source":["def load(paths, verbose=-1):\n","    '''expects images for each class in seperate dir, \n","    e.g all digits in 0 class in the directory named 0 '''\n","    data = list()\n","    labels = list()\n","    # loop over the input images\n","    for (i, imgpath) in enumerate(paths):\n","        # load the image and extract the class labels\n","        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n","        image = np.array(im_gray).flatten()\n","        label = imgpath.split(os.path.sep)[-2]\n","        # scale the image to [0, 1] and add to list\n","        data.append(image/255)\n","        labels.append(label)\n","        # show an update every `verbose` images\n","        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n","            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n","    # return a tuple of the data and labels\n","    return data, labels\n","\n","\n","def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n","    ''' return: a dictionary with keys clients' names and value as \n","                data shards - tuple of images and label lists.\n","        args: \n","            image_list: a list of numpy arrays of training images\n","            label_list:a list of binarized labels for each image\n","            num_client: number of fedrated members (clients)\n","            initials: the clients'name prefix, e.g, clients_1 \n","            \n","    '''\n","\n","    #create a list of client names\n","    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n","\n","    #randomize the data\n","    data = list(zip(image_list, label_list))\n","    random.shuffle(data)\n","\n","    #shard data and place at each client\n","    size = len(data)//num_clients\n","    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n","\n","    #number of clients must equal number of shards\n","    assert(len(shards) == len(client_names))\n","\n","    return {client_names[i] : shards[i] for i in range(len(client_names))}\n","\n","\n","\n","def batch_data(data_shard, bs=32):\n","    '''Takes in a clients data shard and create a tfds object off it\n","    args:\n","        shard: a data, label constituting a client's data shard\n","        bs:batch size\n","    return:\n","        tfds object'''\n","    #seperate shard into data and labels lists\n","    data, label = zip(*data_shard)\n","    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n","    return dataset.shuffle(len(label)).batch(bs)\n","\n","\n","class SimpleMLP:\n","    @staticmethod\n","    def build(shape, classes):\n","        model = Sequential()\n","        model.add(Dense(200, input_shape=(shape,)))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(200))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(classes))\n","        model.add(Activation(\"softmax\"))\n","        return model\n","    \n","\n","def weight_scalling_factor(clients_trn_data, client_name):\n","    client_names = list(clients_trn_data.keys())\n","    #get the bs\n","    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n","    #first calculate the total training data points across clinets\n","    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n","    # get the total number of data points held by a client\n","    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n","    return local_count/global_count\n","\n","\n","def scale_model_weights(weight, scalar):\n","    '''function for scaling a models weights'''\n","    weight_final = []\n","    steps = len(weight)\n","    for i in range(steps):\n","        weight_final.append(scalar * weight[i])\n","    return weight_final\n","\n","\n","\n","def sum_scaled_weights(scaled_weight_list):\n","    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n","    avg_grad = list()\n","    #get the average grad accross all client gradients\n","    for grad_list_tuple in zip(*scaled_weight_list):\n","        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n","        avg_grad.append(layer_mean)\n","        \n","    return avg_grad\n","\n","\n","def test_model(X_test, Y_test,  model, comm_round):\n","    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","    #logits = model.predict(X_test, batch_size=100)\n","    logits = model.predict(X_test)\n","    loss = cce(Y_test, logits)\n","    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n","    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n","    return acc, loss"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8_vcyEXEkQm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}